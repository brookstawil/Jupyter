{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brooks Tawil\n",
    "# ECE: 445 Machine Learning for Engineers\n",
    "## Mini Jupyter Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import mpl_toolkits.mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data\n",
    "Create a matrix A ∈ R\n",
    "3×2 whose individual entries are drawn from a Gaussian distribution with mean\n",
    "0 and variance 1 in an independent and identically distributed (iid) fashion. Once generated, this\n",
    "matrix should not be changed for the rest of this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.77029054 -1.59395621]\n",
      " [-0.03972249 -0.15418255]\n",
      " [-1.1041034  -0.64686915]]\n",
      "A has rank: 2\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "variance = 1\n",
    "ADimensions = (3, 2)\n",
    "A = np.random.normal(mean, variance, ADimensions)\n",
    "print (A)\n",
    "print (\"A has rank: \" + str(np.linalg.matrix_rank(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of Dataset #1\n",
    "Each of our data sample $x ∈ R^3$\n",
    "is going to be generated in the following fashion: $x = Av$, where\n",
    "$v ∈ R^2$ is a random vector whose entries are iid Gaussian with mean 0 and variance 1. Note that we will have a different $v$ for each new data sample (i.e., unlike A, it is not fixed for each data sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is of dimensions: (3, 500)\n",
      "[[ 0.3615011  -0.39714128 -2.23845381 ...  2.43344373  0.48221167\n",
      "   0.15710865]\n",
      " [ 0.00288751 -0.01428247 -0.15117675 ...  0.22413206 -0.02556443\n",
      "  -0.03680563]\n",
      " [ 0.87661455 -0.71025254 -2.39524894 ...  1.2436049   1.83862031\n",
      "   1.24695087]]\n"
     ]
    }
   ],
   "source": [
    "# V is a 2x1 vector, while A is a 3x2 matrix\n",
    "# We need a 3x500 empty matrix to store all of these 500 data samples\n",
    "vDimensions = (2, 1)\n",
    "X = np.array(np.zeros((3,500)))\n",
    "\n",
    "# Multiply A and v_i, store result in the large data matrix X\n",
    "for i in range(500):\n",
    "    v_i = np.random.normal(mean, variance, vDimensions)\n",
    "    x = np.matmul(A, v_i)\n",
    "    for j in range(3):\n",
    "        X[j,i] = x[j]\n",
    "\n",
    "print (\"X is of dimensions: \" + str(X.shape))\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of the matrix resulting from multiplying A and V is equal to 2. The rank of the resulting matrix is bounded by the minimum of the ranks of the matrices A and V. This is becuase the linear transformation of these matrices cannot increase the dimensions of result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X has rank: 2\n"
     ]
    }
   ],
   "source": [
    "print (\"X has rank: \" + str(np.linalg.matrix_rank(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value and Eigenvalue Decomposition of Dataset #1\n",
    "1. Compute the singular value decomposition of X and the eigenvalue decomposition of $XX^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, v = np.linalg.svd(X)\n",
    "w, v = np.linalg.eig(np.matmul(X, np.matrix.transpose(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of $U$ for mthe left singular values of X. We can correspond these to the eignevectors of $XX^T$. The singular values of $X$ are the square roots of the eigenvalues of $XX^T$ as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S: [4.69403497e+01 1.29512543e+01 1.17482077e-15]\n",
      "W: [2.20339643e+03 1.67734987e+02 1.24346750e-14]\n",
      "SQRT(W): [4.69403497e+01 1.29512543e+01 1.11510874e-07]\n"
     ]
    }
   ],
   "source": [
    "print (\"S: \" + str(s))\n",
    "print (\"W: \" + str(w))\n",
    "print (\"SQRT(W): \" + str(np.sqrt(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the energy in $X$, defined by $||X||^2_F$, is equal to the sum of the squares of the singular values of X, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of squares of S: 2371.131415972691\n",
      "Energy of X: 2371.131415972689\n"
     ]
    }
   ],
   "source": [
    "print (\"Sum of squares of S: \" + str(np.sum(np.square(s))))\n",
    "print (\"Energy of X: \" + str(np.sum(np.square(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Since the rank of X is 2, it means that the entire dataset spans only a two-dimensional space in $R^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S: [4.69403497e+01 1.29512543e+01 1.17482077e-15]\n"
     ]
    }
   ],
   "source": [
    "print (\"S: \" + str(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, eventhough we expect to have 2 non-zero values in S with the third value being zero. Yet our third value, while extremely small, is not exactly equal to zero. That may be happening due to the underlying nature of floating point operations. A representation of a floating point number may lead to small errors that result from computations.\n",
    "\n",
    "There is a relationship between the two largest singular values of X and the columns of A, and that is the fact that the sequence of singular values is unique. If the singular values are all distinct, then the sequence of singular vectors is unique also. However, when some set of singular values are equal, the corresponding singular vectors span some subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA of Dataset #1 \n",
    "1. Since each data sample $x_i$ lies in a three-dimensional space, we can have up to three principal components of this data. However, based on your knowledge of how the data was created (and subsequent discussion above), how many principal components should be enough to capture all variation in the data? Justify\n",
    "your answer as much as you can.\n",
    "\n",
    "Based off of the previous work and the generation of the data, we should only need 2 principal components in order to capture the variation in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. While mean centering is an important preprocessing step for PCA, we do not necessarily need to carry\n",
    "out mean centering in this problem since the mean vector will have small entries. Indeed, if we let x1,\n",
    "x2, and x3 denote the first, second, and third component of the random vector x then it follows that\n",
    "E[xk] = 0, k = 1, 2, 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3615011  0.00288751 0.87661455]\n"
     ]
    }
   ],
   "source": [
    "print (X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07280607 -0.0072305  -0.025269  ]\n"
     ]
    }
   ],
   "source": [
    "m = np.array(np.zeros((1,500)))\n",
    "\n",
    "# Take the mean of the columns of X\n",
    "print (np.mean(X, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entries in the above computation are indeed small and mean centering is not necessarily needed in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eigenvalues of the covariance matrix (U_m): \n",
      "[[-0.8240016  -0.55505918 -0.11371312]\n",
      " [-0.06968948 -0.09988543  0.99255543]\n",
      " [-0.56228529  0.82579187  0.04362396]]\n",
      "The top 2 principal components (U_k): \n",
      "[[-0.8240016  -0.55505918]\n",
      " [-0.06968948 -0.09988543]\n",
      " [-0.56228529  0.82579187]]\n"
     ]
    }
   ],
   "source": [
    "# To calculate PCA we need the covariance matrix of X\n",
    "# Then we compute the eigendecomp of the covariance matrix\n",
    "\n",
    "covariance_x = np.cov(X)\n",
    "cov_x_U, cov_x_S, cov_x_V = np.linalg.svd(covariance_x)\n",
    "\n",
    "print (\"The eigenvalues of the covariance matrix (U_m): \\n\" + str(cov_x_U))\n",
    "\n",
    "# From here we must select a certain amopunt of principal components with which to do the transformation.\n",
    "# The problem specifies to select the largest 2 thus we will take the 2 largest columns from the eigenvector matrix\n",
    "\n",
    "cov_x_U_reduced = np.array([cov_x_U[:,0],cov_x_U[:,1]]).transpose()\n",
    "\n",
    "print (\"The top 2 principal components (U_k): \\n\" + str(cov_x_U_reduced))\n",
    "\n",
    "# With the reduced U matrix we can project our original data, X, onto a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 500)\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = np.matmul(cov_x_U_reduced.transpose(), X)\n",
    "\n",
    "# By multiplying the top 2 principal components we have successfully projected the data.\n",
    "# The dimensions of \n",
    "print (feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct (approximate) the original data samples $x_i$ from the PCA feature vectors.\n",
    "\n",
    "We simply take our feature matrix, and multiply by the original principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 500)\n"
     ]
    }
   ],
   "source": [
    "orignal_data = np.matmul(cov_x_U_reduced, feature_matrix)\n",
    "print (orignal_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The representation error between the actual data and the reformed data can be calculated:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
